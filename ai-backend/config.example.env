# =============================================================================
# AI Backend Configuration
# =============================================================================
# Copy this file to .env and modify as needed
# =============================================================================

# Server Configuration
# =============================================================================
API_PORT=5050                    # Default port (used if AUTO_FIND_PORT=false)
HOST=0.0.0.0                     # Server host (0.0.0.0 for all interfaces)
LOG_LEVEL=INFO                   # Logging level (DEBUG, INFO, WARNING, ERROR)

# Dynamic Port Allocation
# =============================================================================
AUTO_FIND_PORT=false             # Set to true to automatically find free port
PORT_RANGE_START=5050            # Start of port range for auto-allocation
PORT_RANGE_END=5100              # End of port range for auto-allocation

# CORS Settings
# =============================================================================
ALLOW_ORIGINS=*                  # Comma-separated list of allowed origins

# Authentication
# =============================================================================
API_KEY=your-secret-api-key-here # API key for authentication
AUTH_REQUIRED=true               # Require API key for all endpoints
METRICS_PUBLIC=false             # Make metrics endpoint public

# Prometheus Metrics
# =============================================================================
PROMETHEUS_ENABLE=true           # Enable Prometheus metrics

# Rate Limiting
# =============================================================================
RATE_LIMIT_PER_MIN=60            # Requests per minute per IP
USE_REDIS=false                  # Use Redis for rate limiting
REDIS_URL=redis://localhost:6379 # Redis connection URL

# vLLM Configuration
# =============================================================================
DEFAULT_MODEL_KEY=               # Default model key for routing
DEFAULT_MODEL_NAME=TinyLlama/TinyLlama-1.1B-Chat-v1.0  # Default model name
VLLM_BASE_URL=http://localhost:8000  # Default vLLM server URL

# Model Routing (optional - for multiple vLLM instances)
# =============================================================================
# MODEL_ROUTE_tinyllama=http://localhost:8000
# MODEL_ROUTE_llama2=http://localhost:8001
# MODEL_ROUTE_mistral=http://localhost:8002

# Allowed Models
# =============================================================================
ALLOWED_MODELS=                  # Comma-separated list of allowed models

# HTTP Timeouts
# =============================================================================
CONNECT_TIMEOUT_SECONDS=10       # Connection timeout
READ_TIMEOUT_SECONDS=60          # Read timeout
WRITE_TIMEOUT_SECONDS=60         # Write timeout
TOTAL_TIMEOUT_SECONDS=180        # Total request timeout

# Database
# =============================================================================
DATABASE_URL=sqlite:///./data/ai_backend.db  # Database connection URL
DB_ECHO=false                    # Echo SQL queries (for debugging)

# Development Settings
# =============================================================================
DEBUG=false                      # Enable debug mode
RELOAD=true                      # Auto-reload on code changes

# =============================================================================
# Usage Examples:
# =============================================================================
# 
# 1. Use fixed port 5050:
#    API_PORT=5050
#    AUTO_FIND_PORT=false
#
# 2. Auto-find port in range 5050-5100:
#    AUTO_FIND_PORT=true
#    PORT_RANGE_START=5050
#    PORT_RANGE_END=5100
#
# 3. Use specific port with fallback:
#    API_PORT=5050
#    AUTO_FIND_PORT=true
#    PORT_RANGE_START=5051
#    PORT_RANGE_END=5100
#
# 4. Development mode:
#    DEBUG=true
#    RELOAD=true
#    LOG_LEVEL=DEBUG
#
# 5. Production mode:
#    DEBUG=false
#    RELOAD=false
#    LOG_LEVEL=INFO
#    AUTH_REQUIRED=true
# =============================================================================
